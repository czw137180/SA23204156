---
title: "Homework"
author: "By SA23204156 程张伟"
date: "2023-09-19"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework-2023.09.18}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
   
## Overview  
   
The content is the homework of the course.

## Question  
  
Exercise 3.2   
   
## Answer   
   
根据密度函数可知拉普拉斯分布的分布函数为
$$
F(x)=\left\{
\begin{aligned}
 \frac12e^x   &&x<0\\
 1-\frac12e^{-x} &&x\geq0\\
\end{aligned}
\right.
$$

根据逆变换法生成样本
```{r,eval=FALSE}
u <- runif(1000) #生成均匀分布样本
x1 <- 1:1000
x1[which(u<0.5)] <- log(2*u[which(u<0.5)])   
x1[which(u>=0.5)] <- -log(2-2*u[which(u>=0.5)])    #利用逆变换得到Laplace分布的样本
```
   
将生成的样本与实际分布比较
```{r,eval=FALSE}
hist(x1, prob = TRUE, main = expression(f(x)==frac(1,2)*e^(-abs(x))))
curve(1/2*exp(-abs(x)), col = "red", add = TRUE, lwd =2)
```
   
   
## Question  
  
Exercise 3.7   
  
## Answer   
   
Beta(a,b)的密度函数为$f(x)=\frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1}$,可以知道$x^{a-1}(1-x)^{b-1}$最大值为$\frac{(a-1)^{a-1}(b-1)^{b-1}}{(a+b-2)^{a+b-2}}$,因此可以选择接受拒绝法中的c为$\frac{1}{B(a,b)}\frac{(a-1)^{a-1}(b-1)^{b-1}}{(a+b-2)^{a+b-2}}$,g(x)选取[0,1]上的均匀分布,从而相应的$\rho(x)=\frac{f(x)}{cg(x)}$.   
   
```{r,eval=FALSE}
my.rbeta <- function(a,b,n){# Beta分布的参数和样本量
  k <- 0                    #k用来控制生成的样本量
  x <- numeric(n)           #用来储存生成的样本
  while (k < n) {
    u <- runif(1) #生成均匀分布
    y <- runif(1) #生成g(x)的样本
    c <- (a-1)^(a-1)*(b-1)^(b-1)/((a+b-2)^(a+b-2))
    rho_y <- y^(a-1)*(1-y)^(b-1) #计算rho(y)
    if(u <= rho_y){  #接受拒绝的过程
      x[k+1] <- y
      k <- k+1
    }
  }
  x                #返回生成的样本
}
```
   
生成Beta(3,2)的样本并比较
```{r,eval=FALSE}
x2 <- my.rbeta(3,2,1000)
hist(x2, prob = T, main = expression(f(x)==frac(1,B(3,2))*x^2*(1-x)))
curve(1/beta(3,2)*x^2*(1-x), col = "red", add = T, lwd =2)
```
   
## Question  
  
Exercise 3.9   
  
## Answer  

```{r,eval=FALSE}
my.repane <- function(n){  #指定样本量
  x <- numeric(n)
  k <- 0
  while (k < n) {    #按照算法得到样本
    k <- k+1
    u <- runif(3, -1, 1)
    if(which.max(abs(u))==3){
      x[k] <- u[2]
    } else 
    {
      x[k] <- u[3]
    }
  }
  x
}
```
   
生成样本并比较
```{r,eval=FALSE}
x3 <- my.repane(1000)
hist(x3, prob = T, main = expression(f(x)==frac(3,4)*(1-x^2)))
curve(3/4*(1-x^2), col = "red", add = T, lwd =2)
```

## Question  
  
Exercise 3.10   
  
## Answer   
   
$pf:$  
$假设生成的随机变量为X,则P(X<=x)=P(X<=x| |U_3|最大)P(|U_3|最大)+P(X<=x| |U_3|不是最大)P(|U_3|不是最大)$   
$由独立性可知P(|U_3|最大)=\frac{1}{3},P(|U_3|不是最大)=\frac{2}{3}$   
$P(X<=x| |U_3|最大)=3P(X<=x,|U_3|最大)=3P(U_2\leq x,|U_3|\geq|U_2|,|U_3|\geq|U_1|)$   
$若x\geq0,$    
$上式=3(\int_{-1}^{-x}\int_{u_3}^{x}\int_{u_3}^{-u_3}+\int_{-x}^{0}\int_{u_3}^{-u_3}\int_{u3}^{-u_3}+\int_{0}^{x}\int_{-u_3}^{u_3}\int_{-u_3}^{u_3}+\int_{x}^{1}\int_{-u_3}^{x}\int_{-u_3}^{u_3})\frac{1}{8}du_1du_2du_3$   
$=\frac{1}{2}+\frac{3}{4}x-\frac{1}{4}x^3$    

$若x<0,$    
$上式=3(\int_{-1}^{x}\int_{u_3}^{x}\int_{u_3}^{-u_3}+\int_{-x}^{1}\int_{-u_3}^{x}\int_{-u_3}^{u_3})\frac{1}{8}du_1du_2du_3$    
$=\frac{1}{2}+\frac{3}{4}x-\frac{1}{4}x^3$  
$综上,P(X<=x| |U_3|最大)=\frac{1}{2}+\frac{3}{4}x-\frac{1}{4}x^3$   
   
$P(X<=x| |U_3|不是最大)=\frac{3}{2}P(X<=x, |U_3|不是最大)=\frac{3}{2}P(U_3\leq x, |U_3|<|U_1|或|U_3|<|U_2|)$   
$由于U_1,U_2地位相同,所以上式=3P(U_3\leq x, |U_3|<|U_1|,|U_1|<|U_2|),这就和第一种情况相同$   
$因此P(X<=x| |U_3|不是最大)=\frac{1}{2}+\frac{3}{4}x-\frac{1}{4}x^3$   
   
$综上,P(X<=x)=\frac{1}{3}(\frac{1}{2}+\frac{3}{4}x-\frac{1}{4}x^3)+\frac{2}{3}(\frac{1}{2}+\frac{3}{4}x-\frac{1}{4}x^3)=\frac{1}{2}+\frac{3}{4}x-\frac{1}{4}x^3$   
   
$对上述分布函数求导可知,f_X(x)=\frac{3}{4}(1-x^2),即为题中所要求的密度函数,证毕.$   

  
## Question  
  
用逆变换法复现放回情况下的sample函数  
  
## Answer   
   
```{r,eval=FALSE}
my.sample <- function(x, size, prob = rep(1,length(x))){
  if(length(prob)!=length(x)){   # 判断变量输入是否合理
    print("The length of x and prob is not equal!")
  }else
  {
    p <- prob/sum(prob)
    cp <- cumsum(p)
    u <- runif(size)      # 生成均匀分布
    r <- x[findInterval(u,cp)+1]    #逆变换
    return(r)         #放回所生成的样本
  }
}
```
   
### 验证函数的正确性:   
未指定概率即默认等概率抽样
```{,eval=FALSE}
x4 <- my.sample(c(1,2,3,4), 1e5)   #不指定权重时生成样本
ct <- as.vector(table(x4))         #统计频率
ct/sum(ct)/rep(0.25,4)             #用比值进行比较每个取值的观测的频率和概率
```
   
指定概率但输入的概率程度与总体可能取值的长度不一致   
```{r,eval=FALSE}
x4 <- my.sample(c(1,2,3,4), 1e5, c(1))
```
   
指定概率且正确输入时   
```{r,eval=FALSE}
prob <- runif(4, 0, 10)  #随机生成权重
p <- prob/sum(prob)      #将权重转换为概率
x5 <- my.sample(c(1,2,3,4), 1e5, prob = prob)   #生成样本
ct <- as.vector(table(x5))          #统计频率
ct/sum(ct)/p             #用比值进行比较每个取值的观测的频率和概率
```
   
从上述的结果可以看出:当输入不当时,能够识别并报错;未指定概率(默认等概率)以及指定概率时,都可以按照我们所要求的概率进行抽样.   
    
   
## Question  
  
投针问题的证明和Monte Carlo模拟  
   
## Answer   
   
$pf:$   
$\hat{p}=\frac{\sum_{i=1}^{m}I(\frac{l}{2}sin(Y)\geq X)}{m},则\hat{p}的渐近方差为\frac{p(1-p)}{m}$   
$由\delta法可知\hat{\pi}的渐近方差为\frac{4l^2(1-p)}{md^2p^3},而p=\frac{2\rho}{\pi}$   
$则可知渐近方差为\frac{\pi^3}{2m}(\frac{1}{\rho}-\frac{2}{\pi})$   
$由于l \leq d,故要是方差最小即\rho最大,故\rho=1$   
   
### 模拟   
   
```{r,eval=FALSE}
l <- c(0.5,0.8,1)
d <- 1
m <- 1e4
v <- numeric(3)
pihat <- numeric(1e6)
for (i in 1:3) {
  for (j in 1:1e6) {
    x <- runif(m,0,d/2)
    y <- runif(m,0,pi/2)
    pihat[j] <- 2*l[i]/d/mean(l[i]/2*sin(y)>x)
  }
  v[i] <- var(pihat)
}
v
```
   
从结果中可以看到在l取0.5,0.8,1的情况下的方差.   

## Question  
  
Exercise 5.6    
control variate approach与常规方法相比的减小的方差比例   
   
## Answer  
  
$和例 5.7中一样,可以知道普通的MC方法得到的方差为Var(g(U))/m,其中Var(g(u))=Var(e^u)=\frac{e^2-1}{2}-(e-1)^2$   
$而Cov(e^u,e^{1-u})=E(e)-E(e^u)E(e^{1-u})=e-(e-1)^2$   
$所以Var(e^u+e^{1-u})=Var(e^u)+V(e^{1-u})+2Cov(e^u,e^{1-u})=e^2-1-2(e-1)^2+2e-2(e-1)^2=-3e^2+10e-5$  
$从而方差减小的比例为[(\frac{e^2-1}{2}-(e-1)^2)-\frac{-3e^2+10e-5}{4}]/(\frac{e^2-1}{2}-(e-1)^2)=98.38\%$   

   
   
   
## Question  
  
Exercise 5.7   
模拟两种方法得到的样本方差减小的比例与理论计算结果比较   
   
## Answer   
   
### 模拟   
```{r,eval=FALSE}
m <- 1e4
U <- runif(m)
T1 <- exp(U)
T2 <- (exp(U) + exp(1-U))/2
mean(T1)
mean(T2)
(var(T1)-var(T2))/var(T1)
```
   
可以看到摸拟的方差减小比例和Exercise 5.6计算结果一致   
    
    
## Question  
   
Proof that $Var(\hat{\theta}^S)/Var(\hat{\theta}^M)\rightarrow0$   
   
## Answer    
   
pf:   
注意到$Var(\hat{\theta}^M)$与层的选取无关,因此只需证明$Var(\hat{\theta}^S)\rightarrow0$    
$而Var(\hat{\theta}^S)=\frac{1}{Mk}\sum_{i=1}^{k}\sigma_i^2$   
$\sigma_i^2=Var[g(U)|I=i]=\int_{a_i}^{b_i}\frac{1}{b_i-a_i}(g(x)-E[g(U)])^2dx\leq N_i^2(其中N_i表示g在第i个区间上最大最小值之差)$   
$而g在[a,b]上是一致连续的,因此\forall\epsilon>0,存在\delta>0,s.t.|x-y|<\delta时,|g(x)-g(y)|<\sqrt{\epsilon}$   
$从而当b_i-a_i<\delta时,N_i^2<\epsilon\Rightarrow Var(\hat{\theta}^S)<\frac{\epsilon}{M}<\epsilon$   
$由\epsilon的任意性我们知道结论成立$   
   
## Question  
   
ex5.13    
Find importance functions $f_1$ and $f_2$ close to $g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-{x^2}/{2}}$, comparing the variance
   
## Answer   
   

$f_1=e^{-(x-1)}$, $(x>1)$   
```{r,eval=FALSE}
m <- 1e6
theta.hat <- se <- numeric(2)
g <- function(x){
  x^2/sqrt(2*pi)*exp(-x^2/2)
}
f1 <- function(x){
  exp(-x+1)
}
x <- 1 + rexp(m)
theta.hat[1] <- mean(g(x)/f1(x))
se[1] <- sd(g(x)/f1(x))
```
   
$f_2=\sqrt{\frac{2}{\pi}}e^{-(x-1)^2/2}$
```{r,eval=FALSE}
f2 <- function(x){
  sqrt(2/pi)*exp(-(x-1)^2/2)
}
x <- 1 + abs(rnorm(m, 1, 1) - 1)
theta.hat[2] <- mean(g(x)/f2(x))
se[2] <- sd(g(x)/f2(x))
```

```{r,eval=FALSE}
integrate(g, 1, Inf)
```
  
```{r,eval=FALSE}
rbind(theta.hat, se)
```
  
实际积分结果为0.400626,表中的估计值也较为接近但可以看出第二种的标准差更小，这是因为我们选取的$f_2$要更为接近$g$.   
   
## Question  
   
ex5.14    
估计$g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-{x^2}/{2}}$的积分
   
## Answer    
   
在上一题中已经对g(x)的积分做了估计   
    
## Question  
   
ex5.15    
用分层的方法估计$\int_0^1\frac{e^{-x}}{1+x^2}dx$,并与不分层的方法比较
   
## Answer    
   
根据例5.13中可知,不分层的结果中$\hat\theta=0.5257801,标准差为0.0970314$.现在将区间分为$((j-1)/5,j/5),j=1,2,3,4,5$    
在每个区间上生成数据的密度为$f_j(x)=e^{-x}/\alpha_j,\alpha_j=(e^{-(j-1)/5}-e^{-j/5})/(1-e^{-1})$    
   
```{r,eval=FALSE}
m <- 1e4
k <- 5
theta.hat2 <- numeric(5)
se2 <- numeric(5)
g2 <- function(x){
  exp(-x)/(1+x^2)
}
f3 <- function(x,j){
  exp(-x)/(exp(-(j-1)/5)-exp(-j/5))
}
for (j in 1:5) {
  u <- runif(m/k)
  x <- -log(exp(-(j-1)/5) - u*(exp(-(j-1)/5)-exp(-j/5)))
  theta.hat2[j] <- mean(g2(x)/f3(x,j))
  se2[j] <- sd(g2(x)/f3(x,j))
}
theta.hat3 <- sum(theta.hat2)
se3 <- sqrt(1/(m*k)*sum(se2^2))
```
   
```{r,eval=FALSE}
rbind(theta.hat3, se3)
```
   
分层的结果估计值依然很准确,但是可以看到标准差明显减小.   
    
## Question  
   
ex6.5    
用MC方法计算t置信区间在卡方分布下的覆盖率
   
## Answer   
   
```{r,eval=FALSE}
n = 20
alpha <- 0.05
UCL <- LCL <- numeric(1000)
for (j in 1:1000) {
  x <- rchisq(n, df = 2)
  UCL[j] <- mean(x) + sd(x)/sqrt(n)*qt(1 - alpha/2, df = n-1)
  LCL[j] <- mean(x) - sd(x)/sqrt(n)*qt(1 - alpha/2, df = n-1)
}
mean(UCL > 2 & LCL < 2)
```
   
从结果可以看出在非正态的情况下覆盖率与0.95有所差距.   
   
## Question  
   
ex6.A    
计算在卡方,均匀和指数分布下t检验的一类错误率是否和名义显著性水平相同
   
## Answer  
  
$卡方分布\mathcal{X}^2(1)$
```{r,eval=FALSE}
error1 <- numeric(3) 
n = 20
alpha <- 0.05
T1 <- numeric(10000)
for (j in 1:10000) {
  x <- rchisq(n, df = 1)
  T1[j] <- sqrt(n)*(mean(x) - 1)/sd(x) 
}
error1[1] <- mean(abs(T1) > qt(1 - alpha/2, df = n-1))
```
   
$均匀分布U(0,2)$
```{r,eval=FALSE}
n = 20
alpha <- 0.05
T2 <- numeric(10000)
for (j in 1:10000) {
  x <- runif(n, 0, 2)
  T2[j] <- sqrt(n)*(mean(x) - 1)/sd(x) 
}
error1[2] <- mean(abs(T2) > qt(1 - alpha/2, df = n-1))
```
  
$指数分布exp(1)$  
```{r,eval=FALSE}
n = 20
alpha <- 0.05
T3 <- numeric(10000)
for (j in 1:10000) {
  x <- rexp(n)
  T3[j] <- sqrt(n)*(mean(x) - 1)/sd(x) 
}
error1[3] <- mean(abs(T3) > qt(1 - alpha/2, df = n-1))
```
   
```{r,eval=FALSE}
rbind(error1)
```
   
以上就是三种分布下的一类错误率,可以看到只有均匀分布的结果与显著性水平较为接近,这可能是因为均匀分布关于均值的对称性所导致的.
    
    
```{r,eval=FALSE}
library(bootstrap)
```

## Question  
   
考虑m=1000个假设,其中前95%为原假设成立,后5%对立假设成立.原假设下p值服从U(0,1),对立假设下为Beta(0.1,1),
分别用Bonferroni和B-H矫正p值,在0.1的显著性水平下进行10000次模拟
   
## Answer
   
```{r,eval=FALSE}
m <- 1000
M <- 10000
FWER.bonf <- FDR.bonf <- TPR.bonf <- numeric(M)
FWER.bh <- FDR.bh <- TPR.bh <- numeric(M)
for (i in 1:M) {
  p <- numeric(1000)
  p[1:950] <- runif(950)
  p[951:1000] <- rbeta(50, 0.1, 1)
  p.bonf <- p.adjust(p, method = 'bonferroni')
  p.bh <- p.adjust(p, method = 'fdr')
  FWER.bonf[i] <- max(p.bonf[1:950] < 0.1)
  FDR.bonf[i] <- sum(p.bonf[1:950] < 0.1)/sum(p.bonf < 0.1)
  TPR.bonf[i] <- sum(p.bonf[951:1000] < 0.1)/50
  FWER.bh[i] <- max(p.bh[1:950] < 0.1)
  FDR.bh[i] <- sum(p.bh[1:950] < 0.1)/sum(p.bh < 0.1)
  TPR.bh[i] <- sum(p.bh[951:1000] < 0.1)/50
}
d <- data.frame(FWER=c(mean(FWER.bonf),mean(FWER.bh)), FDR=c(mean(FDR.bonf),mean(FDR.bh)), TPR=c(mean(TPR.bonf),mean(TPR.bh)))
row.names(d) <- c('Bonf', 'B-H')
d
```

从表格中呈现的结果我们可以看到和预期一致.Bonf的FWER接近0.1,B—H的FWER远大于0.1;Bonf的FWER远小于0.1,B-H的FDR接近0.1;TPR中B-H大于Bonf.   
   
## Question  
   
总体服从指数分布$exp(\lambda)$,$\lambda$的MLE即$\hat{\lambda}=\frac{1}{\overline{X}}$,理论偏差和标准差分别为$\lambda /(n-1)$,$n\lambda/[(n-1)\sqrt{n-2}]$.用Bootstrap方法进行模拟.   

   
## Answer   
   
$\lambda=2,样本量n分别取5,10,20$   
   

```{r,eval=FALSE}
lambda <- 2
B <- 1000
m <- 1000
bias1 <- sd1 <- numeric(m)
bias2 <- sd2 <- numeric(3)
bias3 <- sd3 <- numeric(3)
k <- 1
for (n in c(5,10,20)) {
  for (i in 1:m) {
    B <- 1000
    lambdastar <- numeric(B)
    x <- rexp(n, lambda)
    lambdahat <- 1/mean(x)
    for (j in 1:B) {
      xstar <- sample(x, replace = T)
      lambdastar[j] <- 1/mean(xstar)
    }
    bias1[i] <- mean(lambdastar) - lambdahat
    sd1[i] <- sd(lambdastar)
  }
  bias2[k] <- mean(bias1)
  sd2[k] <- mean(sd1)
  bias3[k] <- lambda/(n-1)
  sd3[k] <- n*lambda/(n-1)/sqrt(n-2)
  k <- k+1
}
d2 <- data.frame(bias.bootstrap = bias2, bias.theoretical = bias3, sd.bootstrap = sd2, sd.theoretical = sd3)
row.names(d2) <- c('n=5', 'n=10', 'n=20')
d2
```
   
从模拟结果来看随着样本量的增大bootstrap的偏差和标准差也越发接近理论的偏差和标准差.   
   
## Question  
   
ex 7.3   
Obtain a bootstrap t confidence interval estimate for the correlation statistic in Example 7.2

   
## Answer    
   
```{r,eval=FALSE}
B <- 200
n <- nrow(law)
R <- numeric(B)
r <- se <- t <- numeric(1000)
corrhat <- cor(law$LSAT, law$GPA)
for (j in 1:1000) {
  for (b in 1:B) {
    i <- sample(1:n, size = n, replace = T)
    LSAT <- law$LSAT[i]
    GPA <- law$GPA[i]
    R[b] <- cor(LSAT, GPA)
  }
  r[j] <- mean(R)
  se[j] <- sd(R)
  t[j] <- (r[j] - corrhat)/se[j]
}
sehat <- mean(se)
QT <- quantile(t, c(0.05/2, 1-0.05/2), type = 1)
names(QT) <- rev(names(QT))
CI <- rev(corrhat - QT*sehat)
CI
```

以上即为所求的t置信区间
   
   
```{r,eval=FALSE}
library('bootstrap')
library('DAAG')
```

## Question  
   
ex 7.5   
用不同的bootstrap置信区间的构造方法对ex.7.4中的数据计算其95%置信区间,并比较区别.

   
## Answer    
```{r,eval=FALSE}
x <- c(3, 5, 7, 18, 43, 85, 91, 98, 100, 130, 230, 487)
SN <- Basic <- Percent <- BCa <- numeric(2)
B <- 10000
theta <- numeric(B)
for (i in 1:B) {
  j <- sample(1:12, 12, replace = T)
  theta[i] <- mean(x[j])
}
y <- sort(theta)
bootse <- sd(theta)
SN <- c(mean(x) - qnorm(0.975)*bootse, mean(x) + qnorm(0.975)*bootse)
Basic <- c(2*mean(x) - y[ceiling(B*0.975)], 2*mean(x) - y[floor(B*0.025)])
Percent <- c(y[floor(B*0.025)], y[ceiling(B*0.975)])
z <- qnorm(mean(theta < mean(x)))
theta.j <- numeric(12)
for (i in 1:12) {
  theta.j[i] <- mean(x[-i])
}
x.j <- mean(theta.j)
a <- sum((x.j-theta.j)^3)/6/sum(abs(x.j-theta.j)^3)
alpha1 <- pnorm(z + (z + qnorm(0.025))/(1 - a*(z + qnorm(0.025))))
alpha2 <- pnorm(z + (z + qnorm(0.975))/(1 - a*(z + qnorm(0.975))))
BCa <- c(y[floor(B*alpha1)], y[ceiling(B*alpha2)])
d <- data.frame('Standard Normal' = SN, Basci = Basic, Percentile = Percent, BCa = BCa)
row.names(d) <- c('LCL', 'UCL')
d
```
   
从结果中我们可以看到不同方法得到的置信区间是不同的,这是由于他们构造方法是基于不同的假设进行的.   
   
## Question  
   
ex 7.8   
用jackknife估计ex 7.7中的偏差和标准差(scor数据集)    

   
## Answer   
  
```{r,eval=FALSE}
n <- nrow(scor)
theta.j <- numeric(n)
for (i in 1:n) {
  Sigma <- cov(scor[-i,])
  e <- eigen(Sigma, symmetric = T)
  lambda <- e$values
  theta.j[i] <- max(lambda)/sum(lambda)
}
e1 <- eigen(cov(scor), symmetric = T)
lambda1 <- e1$values
theta.hat <- max(lambda1)/sum(lambda1)
bias.hat <- (n-1)*(mean(theta.j) - theta.hat)
se.hat <- (n-1)^2/n*var(theta.j)
rbind(bias.hat, se.hat)
```
   
按照Jackknife方法得到的偏差和标准差的估计如上.   
   
## Question  
   
ex 7.11   
在例7.18中,用leave-two-out交叉验证来比较模型       

   
## Answer   
```{r,eval=FALSE}
m <- ironslag$magnetic
c <- ironslag$chemical
n <- length(m)
e1 <- e2 <- e3 <- e4 <- numeric(n*(n-1)/2)
k <- 0
for (i in 1:(n-1)) {
  for (j in (i+1):n) {
    k <- k + 1
    y <- m[-c(i,j)]
    x <- c[-c(i,j)]
    J1 <- lm(y ~ x)
    yhat1 <- J1$coef[1] + J1$coef[2] * c[c(i,j)]
    e1[k] <- sum((m[c(i,j)] - yhat1)^2)
    J2 <- lm(y ~ x + I(x^2))
    yhat2 <- J2$coef[1] + J2$coef[2] * c[c(i,j)] + J2$coef[3] * c[c(i,j)]^2
    e2[k] <- sum((m[c(i,j)] - yhat2)^2)
    J3 <- lm(log(y) ~ x)
    logyhat3 <- J3$coef[1] + J3$coef[2] * c[c(i,j)]
    yhat3 <- exp(logyhat3)
    e3[k] <- sum((m[c(i,j)] - yhat3)^2)
    J4 <- lm(log(y) ~ log(x))
    logyhat4 <- J4$coef[1] + J4$coef[2] * log(c[c(i,j)])
    yhat4 <- exp(logyhat4)
    e4[k] <- sum((m[c(i,j)] - yhat4)^2)
  }
}
cbind(mean(e1), mean(e2), mean(e3), mean(e4))
```
   
从上述结果来看,模型二拟合最好.
   
   
   
## Question  
   
Proof the Stationarity of Metropolis-Hastings sampler Algorithm in continuous situation.

   
## Answer   
$i=j时,显然成立.$   
$i\neq j时,$   
$K(s,r)f(s)=g(r|s)f(s)min\{\frac{f(r)g(s|r)}{f(s)g(r|s)},1\}$   
$=min\{f(r)g(s|r),f(s)g(r|s)\}=g(s|r)f(r)min\{\frac{f(s)g(r|s)}{f(r)g(s|r)},1\}$   
$=K(r,s)f(r)$    
    
    
## Question  
   
ex8.1   Implement the two-sample Cramer-von Mises test for equal distributions   
as a permutation test. Apply the test to the data in Examples 8.1 and 8.2.

   
## Answer   

```{r,eval=FALSE}
f1 <- function(x, y){
  mean(y <= x)
}

cvm <- function(x, y){
  W <- 0
  n <- length(x)
  m <- length(y)
  for (i in 1:n) {
    W <- W + (f1(x[i], x) - f1(x[i], y))^2
  }
  for (j in 1:m) {
    W <- W + (f1(y[j], x) - f1(y[j], y))^2
  }
  m*n/(m+n)^2*W
}
```  
   
```{r,eval=FALSE}
x <- c(158 ,171 ,193 ,199 ,230 ,243 ,248 ,248 ,250 ,267 ,271 ,316 ,327 ,329)
y <- c(141 ,148 ,169 ,181 ,203 ,213 ,229 ,244 ,257 ,260 ,271 ,309)
T.hat <- cvm(x,y)
B <- 1000
z <- c(x,y)
Tstar <- numeric(B)
for (i in 1:B) {
  k <- sample(1:length(z), size = length(x), replace = F)
  x1 <- z[k]
  y1 <- z[-k]
  Tstar[i] <- cvm(x1, y1)
}
p <- mean(c(T.hat, Tstar) >= T.hat)
p
```
   
从上面的估计的p值,我们认为例8.1中的分布差异是不显著的.   
    
## Question  
   
ex8.3    The Count 5 test for equal variances in Section 6.4 is based on the maximum
number of extreme points. Example 6.15 shows that the Count 5 criterion
is not applicable for unequal sample sizes. Implement a permutation test for
equal variance based on the maximum number of extreme points that applies
when sample sizes are not necessarily equal.

    
## Answer    
    
```{r,eval=FALSE}
count5test <- function(x, y){
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  return(max(c(outx, outy)))
}
n1 <- 20
n2 <- 30
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1
m <- 10000
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
count.hat <- count5test(x,y)
z <- c(x,y)
countstar <- numeric(m)
for (i in 1:m) {
  k <- sample(1:length(z), size = length(x), replace = F)
  x1 <- z[k]
  y1 <- z[-k]
  countstar[i] <- count5test(x1,y1)
}
p <- mean(c(count.hat, countstar) >= count.hat)
p
```
     
从置换检验的结果,p值很大.和预期结果一致.
   
    
    
```{r,eval=FALSE}
library("coda")
```

## Question  
   
Consider a model $P(Y = 1 | X1, X2, X3) = \frac{exp(a+b1X1+b2X2+b3X3)}{1+exp(a+b1X1+b2X2+b3X3)}$, where X1 ∼ P(1), X2 ∼ Exp(1)
and X3 ∼ B(1, 0.5).    

   
## Answer   
```{r,eval=FALSE}
f <- function(N, b1, b2, b3, f0){
  x1 <- rpois(N, 1); x2 <- rexp(N); x3 <- rbinom(N, 1, 0.5)
  g <- function(alpha){
    tmp <- exp(-alpha-b1*x1-b2*x2-b3*x3); p <- 1/(1+tmp)
    mean(p) - f0
  }
 solution <- uniroot(g, c(-20,0))
 solution$root
}

f0 <- c(0.1, 0.01, 0.001, 0.0001)
N <- 1e6; b1 <- 0; b2 <- 1; b3 <- -1
a <- numeric(4)
for (i in 1:4) {
  a[i] <- f(N, b1, b2, b3, f0[i])
}

```

```{r,eval=FALSE}
rbind(f0,a)
```
  
```{r,eval=FALSE}
plot(-log(f0), a, pch = 19)
```
   

## Question  
   
ex 9.4   Implement a random walk Metropolis sampler for generating the standard Laplace distribution   

   
## Answer 
```{r,eval=FALSE}
dlaplace <- function(x){
  if(x >=0){
    exp(-x)/2
  } else {
    exp(x)/2
  }
}

rw.Metropolis <- function(sigma, x0, N){
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] > (dlaplace(y)/dlaplace(x[i-1])))
      x[i] <- x[i-1]
    else {
      x[i] <- y
      k <- k + 1
    }
  }
  return(list(x=x, k=k))
}
```
   
```{r,eval=FALSE}
N <- 2000
Sigma <- c(.05, .5, 2, 16)
x0 <- 20
rw1 <- rw.Metropolis(Sigma[1], x0, N)
rw2 <- rw.Metropolis(Sigma[2], x0, N)
rw3 <- rw.Metropolis(Sigma[3], x0, N)
rw4 <- rw.Metropolis(Sigma[4], x0, N)
rbind(Sigma, c(rw1$k, rw2$k, rw3$k, rw4$k)/N)
```


```{r,eval=FALSE}
rw <- cbind(rw1$x, rw2$x, rw3$x, rw4$x)
for (j in 1:4) {
  plot(rw[,j], type = "l",
       xlab = bquote(sigma == .(round(Sigma[j],3))),
       ylab="X", ylim = range(rw[,j]))
}
```
    
从结果可以看出接受率随sigma增大而减小,sigma=2时的链产生情况最好.   
   

## Question  
   
ex 9.7   Implement a Gibbs sampler to generate a bivariate normal chain (Xt, Yt)
with zero means, unit standard deviations, and correlation 0.9   

   
## Answer 
```{r,eval=FALSE}
N <- 4000 
burn <- 1500 
X <- matrix(0, N, 2) 
rho <- 0.9;mu <- c(0,0);Sigma <- c(1,1)
s <- sqrt(1-rho^2)*Sigma
X[1, ] <- c(mu[1], mu[2])
for (i in 2:N) {
x2 <- X[i-1, 2]
m1 <- mu[1] + rho * (x2 - mu[2]) * Sigma[1]/Sigma[2]
X[i, 1] <- rnorm(1, m1, s[1])
x1 <- X[i, 1]
m2 <- mu[2] + rho * (x1 - mu[1]) * Sigma[2]/Sigma[1]
X[i, 2] <- rnorm(1, m2, s[2])
}
b <- burn + 1
x <- X[b:N, ]
```

```{r,eval=FALSE}
plot(x[,1],type='l',col=1,lwd=2,xlab='Index',ylab='Random numbers')
lines(x[,2],col=2,lwd=2)
legend('bottomright',c(expression(X[1]),expression(X[2])),col=1:2,lwd=2)
```

```{r,eval=FALSE}
Y <- x[,2];X <- x[,1]
d1 <- lm(Y~X)
d1$coefficients
```

```{r,eval=FALSE}
qqnorm(d1$residuals)
qqline(d1$residuals)
```
    
从QQ图可以看出,模型的残差符合正态假设.   
    
    
## Question  
   
ex 9.10   Refer to Example 9.1. Use the Gelman-Rubin method to monitor convergence
of the chain, and run the chain until the chain has converged approximately to
the target distribution according to $\hat R$ < 1.2.  

   
## Answer 
```{r,eval=FALSE}
Gelman.Rubin <- function(psi) {
  psi <- as.matrix(psi)
  n <- ncol(psi)
  k <- nrow(psi)
  psi.means <- rowMeans(psi)     
  B <- n * var(psi.means)        
  psi.w <- apply(psi, 1, "var")  
  W <- mean(psi.w)               
  v.hat <- W*(n-1)/n + (B/n)     
  r.hat <- v.hat / W      
  return(r.hat)
}
  
dR <- function(x) {
  if (any(x < 0)) return (0)
  return((x / 4^2) * exp(-x^2 / (2*4^2)))
}

Rayleigh.chain <- function(sigma, x0, N){
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] > (dR(y)/dR(x[i-1])))
      x[i] <- x[i-1]
    else {
      x[i] <- y
    }
  }
  x
}
```

```{r,eval=FALSE}
sigma <- 1; k <- 4; n <- 15000; b <- 1000
x0 <- c(5, 10, 15, 20)
X <- matrix(0, nrow=k, ncol=n)
for (i in 1:k){
  X[i, ] <- Rayleigh.chain(sigma, x0[i], n)
}
psi <- t(apply(X, 1, cumsum))
for (i in 1:nrow(psi))
  psi[i,] <- psi[i,] / (1:ncol(psi))
```

```{r,eval=FALSE}
rhat <- rep(0, n)
for (j in (b+1):n)
  rhat[j] <- Gelman.Rubin(psi[,1:j])
plot(rhat[(b+1):n], type="l", xlab="", ylab="R")
abline(h=1.2, lty=2)
```
     
从结果中可以看到去掉预烧期后链的收敛就已经较好.   
    
```{r,eval=FALSE}
y <- mcmc.list(mcmc(X[1,]), mcmc(X[2,]), mcmc(X[3,]), mcmc(X[4,]))
gelman.diag(y)
```
     
用coda包的得到的结果也说明链收敛的较好.    

  
  
   
```{r,eval=FALSE}
library(boot)
```
## Question  
   
$X_1,\cdots,X_n服从Exp(\lambda).观测值为(u_i,v_i)$   
$(1)分别用极大化似然和EM算法求解\lambda的MLE,证明算法收敛于观测数据的MLE,且有线性收敛速度.$   
$(2)观测值分别为(11,12),(8,9),(27,28),(13,14),(16,17),(0,1),(23,24),(10,11),(24,25),(2,3),变成实现上述方法的数值解.$

   
## Answer   
 
(1)   
$L(\lambda|u_i,v_i)=\Pi_{i=1}^nP_{\lambda}(u_i\leq X_i\leq v_i)=\Pi_{i=1}^n(e^{-\lambda u_i}-e^{-\lambda v_i})$   
$l(\lambda|u_i,v_i)=\sum_{i=1}^n[-\lambda u_i+\log(1-e^{\lambda(u_i-v_i)})]$
$l^\prime(\lambda|u_i,v_i)=\sum_{i=1}^n[-u_i+\frac{(u_i-v_i)\exp(\lambda(u_i-v_i))}{\exp(\lambda(u_i-v_i))-1}]$   
$直接极大化似然函数即l^\prime(\lambda)=0的解$   
$假设真实数据为X^{(m)}=(X^{(m)}_1,\cdots,X^{(m)}_n)$   
$则l(\lambda|u_i,v_i,X^{(m)})=n\log(\lambda)-\lambda\sum_{i=1}^n x^{(m)}_i$   
$E_{\lambda_0}[l(\lambda|u_i,v_i,X^{(m)})|u_i,v_i]=n\log(\lambda)-\lambda\sum_{i=1}^n[u_i+\frac{1}{\hat{\lambda_0}}-\frac{(u_i-v_i)\exp(\hat{\lambda_0}(u_i-v_i))}{\exp(\hat{\lambda_0}(u_i-v_i))-1}]$   
$可知\hat{\lambda_1}=\frac{n}{\sum_{i=1}^n[u_i+\frac{1}{\hat{\lambda_0}}-\frac{(u_i-v_i)\exp(\hat{\lambda_0}(u_i-v_i))}{\exp(\hat{\lambda_0}(u_i-v_i))-1}]}$   
$将上面的\hat{\lambda_0}和\hat{\lambda_1}均替换成\lambda.则可知收敛值应满足与直接极大化似然函数求导后相同的结果，因此EM算法收敛到观测数据的MLE$  
$记f(x)=\frac{n}{\sum_{i=1}^n[u_i+\frac{1}{x}-\frac{(u_i-v_i)\exp(x(u_i-v_i))}{\exp(x(u_i-v_i))-1}]}$
$则f^\prime(\lambda_{\infty})=1-n^{-1}\lambda_{\infty}^2\sum_{i=1}^n\frac{(u_i-v_i)^2\exp(\lambda_{\infty}(u_i-v_i))}{(\exp(\lambda_{\infty}(u_i-v_i)))^2},即0<f^\prime(\lambda_{\infty})<1,因此具有线性收敛速度.$   
   
(2)   
注意到观测到的数据均为$u_i-v_i=-1$,   
$l^\prime(\lambda|u_i,v_i)=-\sum_{i=1}^nu_i-\frac{n\exp(-\lambda)}{\exp(-\lambda)-1}$
```{r,eval=FALSE}
u <- c(11,8,27,13,16,0,23,10,24,2)
t <- sum(u)
f <- function(x) -t-10*exp(-x)/(exp(-x)-1)
s1 <- uniroot(f, interval = c(0.0001,10))
s1$root
```

```{r,eval=FALSE}
lambda0 <- 0
lambda1 <- 1
k <- 0
while (lambda1 - lambda0 > 0.00001 | k<10000) {
  lambda0 <- lambda1
  lambda1 <- 10/(t+10/lambda0+10*exp(-lambda0)/(exp(-lambda0)-1))
    k <- k+1
}
lambda1
```
   
从上面两种方法来看直接求解和利用EM算法求解的结果是一致的.   
    
## Question  
   
ex 11.8   
In the Morra game, the set of optimal strategies are not changed if a constant
is subtracted from every entry of the payoff matrix, or a positive constant
is multiplied times every entry of the payoff matrix. However, the simplex
algorithm may terminate at a different basic feasible point (also optimal).
Compute B <- A + 2, find the solution of game B, and verify that it is one
of the extreme points (11.12)–(11.15) of the original game A. Also find the
value of game A and game B

   
## Answer    
   
```{r,eval=FALSE}
solve.game <- function(A) {
min.A <- min(A)
A <- A - min.A 
max.A <- max(A)
A <- A / max(A)
m <- nrow(A)
n <- ncol(A)
it <- n^3
a <- c(rep(0, m), 1) 
A1 <- -cbind(t(A), rep(-1, n)) 
b1 <- rep(0, n)
A3 <- t(as.matrix(c(rep(1, m), 0))) 
b3 <- 1
sx <- simplex(a=a, A1=A1, b1=b1, A3=A3, b3=b3,
maxi=TRUE, n.iter=it)

a <- c(rep(0, n), 1) 
A1 <- cbind(A, rep(-1, m)) 
b1 <- rep(0, m)
A3 <- t(as.matrix(c(rep(1, n), 0))) 
b3 <- 1
sy <- simplex(a=a, A1=A1, b1=b1, A3=A3, b3=b3,
maxi=FALSE, n.iter=it)
soln <- list("A" = A * max.A + min.A,
"x" = sx$soln[1:m],
"y" = sy$soln[1:n],
"v" = sx$soln[m+1] * max.A + min.A)
soln
}
```
   
```{r,eval=FALSE}
A <- matrix(c( 0,-2,-2,3,0,0,4,0,0,
               2,0,0,0,-3,-3,4,0,0,
               2,0,0,3,0,0,0,-4,-4,
               -3,0,-3,0,4,0,0,5,0,
               0,3,0,-4,0,-4,0,5,0,
               0,3,0,0,4,0,-5,0,-5,
               -4,-4,0,0,0,5,0,0,6,
               0,0,4,-5,-5,0,0,0,6,
               0,0,4,0,0,5,-6,-6,0), 9, 9)
s <- solve.game(A+3)
round(cbind(s$x, s$y), 7)
```
   
从结果可以看出在将A替换成A+3后结果依然是(0, 0, 25/61, 0, 20/61, 0, 16/61, 0, 0)的形式.   
   
```{r,eval=FALSE}
s2 <- solve.game(A)
rbind(A = s2$v,B = s$v)
```
   
游戏A的value为0;将收益+3后,游戏B的收益相应的变为了3.   
   
   
   
```{r,eval=FALSE}
library(Rcpp)
library(microbenchmark)
```

## Question  
   
2.1.3  Exercise 4
Why do you need to use unlist() to convert a list to an atomic
vector? Why doesn’t as.vector() work?
   
## Answer    
   
将列表转换成向量可以进行向量方面的处理;as.vector函数是将矩阵转换成向量,元素是数,但列表里面的元素更复杂且长度不一定一致.   
   
    
## Question  
   
2.3.1 Exercise 1   
What does dim() return when applied to a vector?
   
## Answer  
    
NULL    
    
    
## Question  
   
2.3.1 Exercise 2   
If is.matrix(x) is TRUE, what will is.array(x) return?
   
## Answer    
    
TRUE   
   
## Question  
   
2.4.5 Exercise 2   
What does as.matrix() do when applied to a data frame with
columns of different types?
   
## Answer    
   
所有的列都会转换成同一种数据类型.   
   
## Question  
   
2.4.5 Exercise 3   
Can you have a data frame with 0 rows? What about 0
columns?

   
## Answer   
   
## 0行   
```{r,eval=FALSE}
d <- data.frame(x = c(1), y = c(1), z = c(1))
d <- d[-1,]
d
```
   

## 0列   
```{r,eval=FALSE}
d <- data.frame(x = c(1,2,3))
d <- d[,-1]
d
```
   
## Question  
   
11.1.2 Exercise 2   
Can you have a data frame with 0 rows? What about 0
columns?

   
## Answer  

```{r,eval=FALSE}
scale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}
```
   
应用到每一列
```{r,eval=FALSE}
d1 <- data.frame(x = c(1,2,3), y = c(2,3,6))   
lapply(d1, scale01)
```
   
应用到每一数值的列   
```{r,eval=FALSE}
d2 <- data.frame(x = c(1,2,3), y = c(2,3,6), z = c("a", "b", "c"))
lapply(d2[,which(unlist(lapply(d2, is.numeric)))], scale01)   #对每一列先应用is.numeric函数,再对合适的列用scale01函数
```
   

    
## Question  
   
11.2.5 Exercise 1   
Can you have a data frame with 0 rows? What about 0
columns?

   
## Answer  
  
应用到每一列
```{r,eval=FALSE}
d1 <- data.frame(x = c(1,2,3), y = c(2,3,6))   
vapply(d1, sd, c(sd = 0))
```
   
应用到每一数值的列   
```{r,eval=FALSE}
d2 <- data.frame(x = c(1,2,3), y = c(2,3,6), z = c("a", "b", "c"))
vapply(d2[,which(unlist(vapply(d2, is.numeric, c(sd=0)))==1)], sd, c(sd=0))   #对每一列先应用is.numeric函数,再对合适的列用scale01函数
```


## Question  
   
对exercise 9.8分别编写r函数和rcpp函数比较速度   


## Answer   
   
r function
```{r,eval=FALSE}
fr <- function(a, b, n, N){
  X <- matrix(0, N, 2)
  X[1, ] <- c(1, 1/2)
  for (i in 2:N) {
    y <- X[i-1, 2]
    X[i,1] <- rbinom(1, n, y)   #更新一个变量
    x <- X[i,1]
    X[i,2] <- rbeta(1, a, b)    #更新另一个变量
  }
  return(X)
}
```

```{r,eval=FALSE}
cppFunction('NumericMatrix fc(float a, float b, int n, int N){;
            NumericMatrix X(N,2);
            X[1,1] = 1;
            X[1,2] = 1/2;
            for(int i = 2; i <= N; i++){
              float y = X[i-1,2];
              NumericVector x = rbinom(1, n, y); 
              X[i,1] = x[1];
              NumericVector z = rbeta(1, a, b);
              X[i,2] = z[1];
            }
            return(X);
}')
```

```{r,eval=FALSE}
ts <- microbenchmark(fR = fr(2, 2, 10, 1000), fC = fc(2, 2, 10, 1000))
summary(ts)[,c(1,3,5,6)]
```
   
从结果可以看出用cpp的时间大大减少.    
   
   
